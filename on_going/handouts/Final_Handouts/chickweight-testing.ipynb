{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breaking Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://koaning.io/old/theme/data/chickweight.csv'\n",
    "chickweight_df = (pd.read_csv(url).rename(str.lower, axis='columns'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume our chick weight dataset is a huge production database that changes continuously. We're writing some analysis pipelines based on a small extract from this database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chickweight_test_df = chickweight_df.loc[1:10]\n",
    "chickweight_test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weak Links in a Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions below are used to select \"overweight\" chickens, given the time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_time_z_value(chick_weight_df):\n",
    "    return (\n",
    "        chick_weight_df\n",
    "        .assign(weight_time=lambda x: x['weight'] / x['time'])\n",
    "        .assign(weight_time_z=lambda x: (x['weight_time'] - x['weight_time'].mean()) / x['weight_time'].std())\n",
    "        .drop('weight_time', axis=1)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_overweight(chick_weight_df, z_threshold=0):\n",
    "    return (\n",
    "        chick_weight_df\n",
    "        .pipe(weight_time_z_value)\n",
    "        .loc[lambda x: x['weight_time_z'] > z_threshold]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_overweight(chickweight_test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Besides the meaninglessnes of this analysis, what can go wrong when performing this selection? I.e. how can we (obviously) manipulate the input dataframe to let it crash?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load answers/breaking_with_exception_missing_column.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** There's a more subtle bug in this code. While it will not lead to exceptions, it will surely lead to unexpected results. What input data can be added to our test dataframe to make this analysis even more useless than it already is?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load answers/chickweight_test_unexpected_results_call.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are examples of ad-hoc, informal testing of our functions. We intuitively feel that the last result is unexpected. Regardless how we intend to fix our problem, we encountered an *undocumented assumption* about our code. Instead of documenting the functionality in prose, let's do it in code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestSelectOverweight(unittest.TestCase):\n",
    "    def test_select_overweight_default(self):\n",
    "        test_df = pd.DataFrame({\n",
    "            'chick': [1, 2],\n",
    "            'weight': [100, 200],\n",
    "            'time': [1, 1]\n",
    "        })\n",
    "        result_df = select_overweight(test_df)\n",
    "        self.assertEqual(1, len(result_df))\n",
    "        self.assertEqual(2, result_df.chick.values[0])\n",
    "    \n",
    "    def test_select_overweight_zero_time(self):\n",
    "        test_df = pd.DataFrame({\n",
    "            'chick': [1, 2, 3],\n",
    "            'weight': [100, 200, 50],\n",
    "            'time': [1, 1, 0]\n",
    "        })\n",
    "        result_df = select_overweight(test_df)\n",
    "        self.assertEqual(1, len(result_df))\n",
    "        self.assertEqual(2, result_df.chick.values[0])\n",
    "\n",
    "unittest.main(argv=['ignored', '-v', 'TestSelectOverweight'], exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The documentation for Python's standard unit testing framework can be found [here](https://docs.python.org/3/library/unittest.html). There are many more assertions and checks than `assertEqual()` in the above example. Besides, pandas and numpy have their own specialized assertion functions in the [`pandas.testing`](https://pandas.pydata.org/pandas-docs/stable/reference/general_utility_functions.html#testing-functions) and [`numpy.testing`](https://docs.scipy.org/doc/numpy/reference/routines.testing.html) packages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** We discover a failure in our (production) code, and find its root cause. What's (often) the best first action to take?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What to Test?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume that the chicks's health was monitored during the experiment, and recorded in a new `sick` feature. According to the experiment manual, a value of `0` means that the chick was healthy. However, values for a non-healthy state were not clearly defined and may range from number of treatments given to a textual description of any such treatment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chickweight_test_df = chickweight_test_df.assign(sick=[0, 0, 0, 0, 0, 0, 0, 1, 'antibiotics', 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chickweight_test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to know the average weight gain for a chick only when it is healthy, the following might be a solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_gain_when_healthy(chickweight_df):\n",
    "    return(\n",
    "        chickweight_df\n",
    "        .assign(weight_diff=lambda x: x['weight'].diff())\n",
    "        .loc[lambda x: ~(x['sick'].astype(bool)), 'weight_diff']\n",
    "        .mean()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_gain_when_healthy(chickweight_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestWeightGainWhenHealthy(unittest.TestCase):\n",
    "    def test_weight_gain_when_healthy_default(self):\n",
    "        test_df = pd.DataFrame({\n",
    "            'weight': [100, 105, 120, 120, 120],\n",
    "            'sick': [0, 0, 0, 1, 1],\n",
    "            'time': [0, 1, 2, 3, 4]\n",
    "        })\n",
    "        result = weight_gain_when_healthy(test_df)\n",
    "        self.assertEqual(10, result)\n",
    "\n",
    "unittest.main(argv=['ignored', '-v', 'TestWeightGainWhenHealthy'], exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, so good. However, it turns out that sometimes the experimenters didn't bother to fill out the health checks in the case that chicks were healthy. Will this impact our existing function (and tests)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assignment:** Extend the above test case by adding tests that break our `weight_gain_for_sick()` function (i.e. let it return unexpected results), and improve that function such that it can handle these edge cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load answers/weight_gain.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most often, failures - ranging from obvious exceptions to very subtle (but equally damaging) unexpected outcomes - are caused by not properly dealing with:\n",
    "\n",
    "- Missing values\n",
    "- Numerical values of 0\n",
    "- Empty strings\n",
    "- Complex branching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking Common Assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [first problem we ran into](http://localhost:8888/notebooks/labs/chickweight/chickweight-testing.ipynb#Weak-Links-in-a-Pipeline) was caused by unexpectedly missing a crucial column in a dataframe.\n",
    "\n",
    "**Q:** How can this issue be handled? Is the function `weight_time_z_value()` responsible for gracefully returning anything (e.g. an empty dataframe or a copy of the input)? What makes sense?\n",
    "\n",
    "Similarly, the function `weight_gain_when_healthy()` assumes that the input data is sorted on the `time` column. Such assumptions occur often.\n",
    "\n",
    "**Q:** Should we clutter our pipeline functions (+ their corresponding unit tests!) with code for checking existence of certain columns, or sorted-ness of other columns? What are the alternatives?\n",
    "\n",
    "**Assignment:** Create a function that checks if a dataframe passed to any of our pipeline functions contains a given column and raises an exception if it doesn't. Hints:\n",
    "\n",
    "1. How did we previously create a single function that could log the size of dataframes passed to pipeline functions?\n",
    "2. Similar to unit testing assertions, Python has the [`assert`](https://docs.python.org/3/reference/simple_stmts.html#the-assert-statement) statement.\n",
    "\n",
    "**Assignment:** Extend the above function with an argument that indicates whether a more graceful way should be used to deal with missing columns, e.g. returning the input dataframe as-is.\n",
    "\n",
    "**Assignment:** Extend the above function with an argument that indicates whether a given column is expected to be sorted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load answers/common_assumptions.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
