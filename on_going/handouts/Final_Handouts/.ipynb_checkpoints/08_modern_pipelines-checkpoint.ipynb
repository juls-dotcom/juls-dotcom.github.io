{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modern Pipelines\n",
    "\n",
    "In the previous notebooks we've discussed a lot about what pandas has to offer. It offers a lot of details but it hasn't discussed yet how you may want to use pandas in practice. In this notebook we will demonstrate a problem with certain workflows and we will conclude with a workflow that should be adopted instead. \n",
    "\n",
    "## Why it Matters \n",
    "\n",
    "A lot of notebooks become these long scripts of scrolling and it might get hard to figure out what you're doing. It also makes it harder for others to pick up. You want to make clean code, this needs to be a base attitude.\n",
    "\n",
    "## Bad Code Smells\n",
    "\n",
    "Let's demonstrate some bad pandas code and let's also talk about **why** it is bad. We'll start with some imports and a `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"v1\": [1,2,3,4,5,6,7,8], \"v2\": [8,7,6,5,4,3,2,1]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose now that I want to filter out rows, this bit of code will do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df[\"v1\"] % 2) == 0][df[\"v2\"] <= 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is fine, but we do get a warning. You can choose to ignore the warning, but something that is potentially very dangerous is happening here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df[\"v1\"] % 2) == 0].reset_index()[df[\"v2\"] <= 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we reset the index, then suddenly our filtering no longer works. Pandas is trying to help you under the hood to make sure that this error does not happen, but from a mechanical perspective this is a bit crazy. We are trying to merely filter on the **values** of a column. The index should have no influence. \n",
    "\n",
    "There's other things that you might be tempted to do that make this way of \"writing pandas\" a bit tricky. You may have seen code like this instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[(df[\"v1\"] % 2) == 0]\n",
    "df = df.reset_index()\n",
    "df = df[df[\"v2\"] <= 3]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, this is suboptimal. For starters, we gain a column `index` that nobody wanted and also (and this is the part that might cause errors) we are overwriting our raw data. Overwriting raw data makes it very hard to reproduce steps and it makes it even more tricky to find bugs. This is especially true when you are writing long notebooks.\n",
    "\n",
    "Another alternative is that you might consider writing something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"v1\": [1,2,3,4,5,6,7,8], \"v2\": [8,7,6,5,4,3,2,1]})\n",
    "df2 = df[(df[\"v1\"] % 2) == 0]\n",
    "df3 = df2.reset_index()\n",
    "df4 = df3[df[\"v2\"] <= 3]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this also leads to junk code. Knowing the difference between `df2` and `df4` is still an effort.\n",
    "\n",
    "# A Path Forward \n",
    "\n",
    "Maybe we should lay down the ground rules for writing pandas. \n",
    "\n",
    "1. Operations should never change the original dataset. \n",
    "2. We need to be able to seperate concerns in our code so that we don't just want to know **what** code is running but also **when**. \n",
    "4. We want to make small changes easily to test the effects of certain parameters in our analysis. \n",
    "\n",
    "Let's adress these three points.\n",
    "\n",
    "## Point 1: Never Change the Original Data\n",
    "\n",
    "The code we had before can be rewritten as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"v1\": [1,2,3,4,5,6,7,8], \"v2\": [8,7,6,5,4,3,2,1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df\n",
    " .loc[lambda d: (d[\"v1\"] % 2 == 0)]\n",
    " .loc[lambda d: d[\"v2\"] <= 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice that the original dataset has not changed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another benefit of this way of writing code is that you can clearly see from top to bottom from left to right what is happening. \n",
    "\n",
    "You might be tempted to think that you are limited by this way of writing code, but you actually get to still do nearly everything. \n",
    "\n",
    "- add/overwrite columns `.assign()`\n",
    "- filter rows `.loc[]`\n",
    "- make a grouped object `.groupby()`\n",
    "- shorthand aggregation for groupby `.agg()`\n",
    "- general aggregation for groupby `.apply()`\n",
    "- sorting rows `.sort_values()`\n",
    "- reset the index `.reset_index()`\n",
    "- select top/bottom rows `.head()/.tail()`\n",
    "\n",
    "Note that all these methods do not change the original dataset as base behaviour.\n",
    "\n",
    "An example of a set of chainable commands is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df\n",
    " .loc[lambda d: (d[\"v1\"] % 2 == 0)]\n",
    " .assign(v3 = lambda d: d[\"v1\"]*d[\"v2\"])\n",
    " .loc[lambda d: d[\"v3\"] >= 10]\n",
    " .head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Point 2: Seperate Concerns\n",
    "\n",
    "We can expand this lesson to another level of abstraction. Let's pretend that we've read in a timeseries and that this is the new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ts_df():\n",
    "    dates = [str(_) for _ in pd.date_range(\"2018-01-01\", \"2019-01-01\")]\n",
    "    values = [np.nan if np.random.random() < 0.05 else _ for _ in np.random.normal(0, 1, 366)]\n",
    "    return pd.DataFrame({\"date\": dates, \"value\": values})\n",
    "\n",
    "date_df = make_ts_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This `date_df` needs parsing before we can do something useful with it. In particular:\n",
    "\n",
    "- we need to make sure that the types are set \n",
    "- we might want to clean the `nan` values\n",
    "- potentially we also want to remove outliers. \n",
    "\n",
    "This gives us to opportunity to learn the most marvelous and stunning method in all of the pandas library. Pay close attention. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_types(dataf):\n",
    "    return (dataf\n",
    "            .assign(date=lambda d: pd.to_datetime(d.date)))\n",
    "\n",
    "def clean_nan(dataf):\n",
    "    return (dataf.dropna())\n",
    "\n",
    "def remove_outliers(dataf):\n",
    "    return (dataf\n",
    "            .loc[lambda d: d.value > -2.0]\n",
    "            .loc[lambda d: d.value < 2.0])\n",
    "\n",
    "prep_df = (date_df\n",
    "           .pipe(parse_types)\n",
    "           .pipe(clean_nan)\n",
    "           .pipe(remove_outliers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.pipe()` method allows us to pass a function that accepts a dataframe as it's first argument. This is a very nice flow. Note:\n",
    "\n",
    "- We can give the function a descriptive name and on a pipeline level this allows us to see \"what\" is happening \"when\". \n",
    "- If there is ever a bug this pipeline will make it easier for us to figure out where it is. Since every step is merely a function. \n",
    "- We can write unit tests for these small pipeline steps such that we can test for expected behavior. \n",
    "- We can automate logging a bit. \n",
    "\n",
    "To demonstrate this last point. Let's add a decorator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import wraps\n",
    "\n",
    "\n",
    "def log_pandas_pipefunc(func):\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        result = func(*args, **kwargs)\n",
    "        shape_before = args[0].shape\n",
    "        shape_after = result.shape\n",
    "        print(f\"{func.__name__} => before shape:{shape_before} after shape:{shape_after}\")\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "@log_pandas_pipefunc\n",
    "def parse_types(dataf):\n",
    "    return (dataf\n",
    "            .assign(date=lambda d: pd.to_datetime(d.date)))\n",
    "\n",
    "@log_pandas_pipefunc\n",
    "def clean_nan(dataf):\n",
    "    return (dataf.dropna())\n",
    "\n",
    "@log_pandas_pipefunc\n",
    "def remove_outliers(dataf):\n",
    "    return (dataf\n",
    "            .loc[lambda d: d.value > -2.0]\n",
    "            .loc[lambda d: d.value < 2.0])\n",
    "\n",
    "prep_df = (date_df\n",
    "           .pipe(parse_types)\n",
    "           .pipe(clean_nan)\n",
    "           .pipe(remove_outliers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the benefit of having a standard decorator that can log pandas steps: \n",
    "\n",
    "1. When writing code, this might help you in discovering what is happening. If you see rows dissapear while they shouldn't this log might give you a proxy. \n",
    "2. When this pandas code goes to production you will have some logging for free in airflow. If something goes wrong there you may also be able to debug more easily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caveats \n",
    "\n",
    "We should be careful when we are writing `.pipe`-lines. The function going into a `.pipe()` might not be stateless. Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_columns(dataf):\n",
    "    dataf.columns = [\"a\", \"b\"]\n",
    "    return dataf \n",
    "\n",
    "date_df = make_ts_df()\n",
    "date_df.pipe(rename_columns).columns, date_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In such a situation it is best to include a `.copy()` command. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_columns(dataf):\n",
    "    dataf = dataf.copy()\n",
    "    dataf.columns = [\"a\", \"b\"]\n",
    "    return dataf \n",
    "\n",
    "date_df = make_ts_df()\n",
    "date_df.pipe(rename_columns).columns, date_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be careful with this. We want our functions to be stateless or otherwise we lose our benefits.\n",
    "\n",
    "## Point 3: Abstraction on Higher Levels\n",
    "\n",
    "To fully appreciate what the pandas pipelines can do let us rewrite one function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@log_pandas_pipefunc\n",
    "def remove_outliers(dataf, min_value=-2.0, max_value=2.0):\n",
    "    return (dataf\n",
    "            .loc[lambda d: d.value > min_value]\n",
    "            .loc[lambda d: d.value < max_value])\n",
    "\n",
    "prep_df = (date_df\n",
    "           .pipe(parse_types)\n",
    "           .pipe(clean_nan)\n",
    "           .pipe(remove_outliers, max_value=0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.pipe()` can accept keyword arguments. This allows you to change, say, threshold values on a high level. No need to change the original function, you can change things from a higher level. This is great because it will encourage you to write functions that are general. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion \n",
    "\n",
    "> **\"Pipelines are the only correct way to write pandas.\"** - Vincent D. Warmerdam \n",
    "\n",
    "This is a bold statement, but some collegues of us feel very strongly in this. \n",
    "\n",
    "Even if you take this statement with a grain of salt, it is important to write your code in such a way that your notebook remains clear. Take this serious. If it takes a lot of effort to understand the code of your colleagues, then your team will be slower than you want it to be. \n",
    "\n",
    "A notebook is a great scratchpad, but that is no excuse to write unclear code."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
