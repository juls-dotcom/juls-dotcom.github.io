{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Utilities in Pandas \n",
    "\n",
    "It is worth to mention that pandas has some *amazing* utilities when dealing with timestamps. In this notebook we will demonstrate some of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "dates = pd.date_range(\"2015-01-01\", \"2018-01-01\")\n",
    "values = np.random.normal(0, 1, len(dates)).cumsum()\n",
    "df = pd.DataFrame({\"dates\": dates, \"values\": values}).set_index(\"dates\")\n",
    "df.plot(figsize=(16,4));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Easy Aggregations \n",
    "\n",
    "If you have a dataframe that has a datetime-index you can use the `.resample()` method to perform a \"groupby\"-like grouping based on the index.\n",
    "\n",
    "For example, we can easily calculate the mean per year by running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.resample(\"Y\").mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also run the same aggregation per month `M`, week `W` or quarter `Q`. If the index is a datetime stamp that also includes times then you can aggregate per hour. The script below demonstrates this by calulating the mean per hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seconds = pd.date_range(\"2015-01-01 00:00:00\", \"2015-01-02 00:00:00\", freq=\"s\")\n",
    "values_s = np.random.normal(0, 1, len(seconds)).cumsum()\n",
    "df_seconds = pd.DataFrame({\"time\": seconds, \"value\": values_s}).set_index(\"time\")\n",
    "df_seconds.resample(\"H\").mean().head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we can also use a general `.apply()` or `.transform` method here as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_seconds\n",
    " .resample(\"H\")\n",
    " .apply(lambda d: pd.Series({\n",
    "     \"mean_value\": d['value'].mean(), \n",
    "     \"var_value\": d['value'].var()\n",
    " }))\n",
    " .head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_seconds\n",
    " .resample(\"H\")\n",
    " ['value']\n",
    " .transform(np.mean)\n",
    " .reset_index()\n",
    " .head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Based Features \n",
    "\n",
    "Any column in pandas that is of dtype `datetime` has a module attached that can be used to perform vectorised datetime operations. This is very similar to the `.str` module attached to string columns. It is a good thing to explore since the alternative is non-vectorised and much slower.\n",
    "\n",
    "Below is an example of getting the `weekofyear`. Feel free to explore other properties and methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = pd.date_range(\"2015-01-01\", \"2018-01-01\")\n",
    "date_column = pd.DataFrame({\"date\": dates})['date']\n",
    "date_column.dt.weekofyear.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rolling and Smoothing\n",
    "\n",
    "Sometimes you might want to create a rolling average. Pandas also supports this via the `.rolling()` method which can be called both on a dataframe as well as a series object. It can be used to calculate multiple properties too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = pd.date_range(\"2015-01-01\", \"2018-01-01\")\n",
    "values = np.random.normal(0, 1, len(dates)).cumsum()\n",
    "df = pd.DataFrame({\"dates\": dates, \"values\": values}).set_index(\"dates\")\n",
    "df.assign(rolling_mean = lambda d: d['values'].rolling(20).mean()).plot(figsize=(16, 4));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this rolling property can be centered but can also be tasked with taking a rolling value over a week; pandas is able to recognize datetime values in the index on which to base the roller. \n",
    "\n",
    "You can see the effect of the centering below, pay attention to the fact that the green line does not lag anymore. Also note that in order to get this propery we need information from the future so the green line is a bit naughty to use in timeseries predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df\n",
    " .assign(rolling_mean_d = lambda d: d['values'].rolling(\"30D\").mean())\n",
    " .assign(rolling_mean_center = lambda d: d['values'].rolling(30, center=True).mean())\n",
    ").plot(figsize=(16,4));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also note that you can do more than just \"calculating the mean\" you can also compute other statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df\n",
    " .assign(rolling_mean_d = lambda d: d['values'].rolling(\"30D\").apply(np.var, raw=True))\n",
    " .assign(rolling_mean_center = lambda d: d['values'].rolling(30, center=True).var())\n",
    ").plot(figsize=(16,4));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative to calculating the rolling statistics is to smooth the timeseries exponentially with the following formula:\n",
    "\n",
    "$$\\hat{y_t} = \\alpha y_t + (1-\\alpha) \\hat{y_{t-1}}$$\n",
    "\n",
    "The idea is to recursively smooth the series by averaging the current average with the current value. If the alpha value is high then the smoothing will be low but the average can respond quicker to changes and if it is low it will result in something much more flat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df\n",
    " .assign(smoothed1=lambda d: d['values'].ewm(alpha=0.01).mean())\n",
    " .assign(smoothed2=lambda d: d['values'].ewm(alpha=0.1).mean())\n",
    ").plot(figsize=(16, 4));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fill NA\n",
    "\n",
    "Note that these smoothing functions can be especially nice when you have missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "df_nan = (df\n",
    "          .head(40)\n",
    "          .assign(missing=lambda d: [_ if random.random() < 0.6 else np.nan for _ in d['values']])\n",
    "          .assign(smooth=lambda d: d['values'].ewm(alpha=0.5).mean().fillna(method=\"ffill\")))\n",
    "\n",
    "plt.figure(figsize=(16, 4))\n",
    "plt.subplot(121)\n",
    "plt.scatter(range(len(df_nan)), df_nan['values'])\n",
    "plt.scatter(range(len(df_nan)), df_nan['missing'], c='red')\n",
    "plt.title(\"the red values are missing\");\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.scatter(range(len(df_nan)), df_nan['values'])\n",
    "plt.scatter(range(len(df_nan)), df_nan['smooth'], c='red')\n",
    "plt.title(\"the red values are interpolated\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Pandas\n",
    "\n",
    "One interpretation of a rolling window is that you are smoothing the original timeseries; in other words we might be 'de-noiseing' the dataset. One advanced setting that is worth mentioning is the window type. By setting the window type to be \"gaussian\" you can make the smoothing weighted. This way points that are further away have less influence. Another interpretation of this method is that we apply a convolution on the timeseries.\n",
    "\n",
    "Extra documentation on this topic can be found [here](http://pandas.pydata.org/pandas-docs/stable/user_guide/computation.html#rolling-windows) and a demo can be seen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df\n",
    " .assign(rolling_mean_d = lambda d: (d['values']\n",
    "                                     .rolling(30, center=True, win_type=\"gaussian\")\n",
    "                                     .mean(std=5)))\n",
    ").plot(figsize=(16, 4));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expanding NA\n",
    "\n",
    "A final method worth mentioning is `.expanding()`. In essense this allows you to write functions like `cumsum()` but with more customisation options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df\n",
    " .assign(cumsum=lambda d: d['values'].cumsum())\n",
    " .assign(expanding=lambda d: d.expanding()['values'].sum())\n",
    " .head(6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can write your own aggregation functions as you wish, to show what to expect in the `apply()` we print the results below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_and_mean(d):\n",
    "    print(d)\n",
    "    return np.mean(d)\n",
    "\n",
    "df.head(4).expanding().apply(print_and_mean)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
