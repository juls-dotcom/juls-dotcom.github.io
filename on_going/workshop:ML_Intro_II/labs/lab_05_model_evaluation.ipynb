{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 05 - Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn import datasets\n",
    "%matplotlib inline\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from lib.processing_functions import convert_to_pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise goals:\n",
    "\n",
    "- see the full flow going from model selection, evaluation to persistence\n",
    "- get some practice with classification metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 1: Persist the best model\n",
    "\n",
    "We will optimize and evaluate the performance of multiple regression models on the Boston dataset and store the best model on disk. The model that will considered are `LinearRegression`, `Lasso` and `Ridge`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the Boston dataset\n",
    "X, y = convert_to_pandas(datasets.load_boston())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Prepare models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using grid search for optimizing the Lasso and Ridge models, we will use the model versions with built-in cross-validated parameter optimization.\n",
    "\n",
    "In Lab 1 we saw that the features vary a lot in their spread. However, Lasso and Ridge regression usually perform best when all features are scaled to the same range. The Ridge and Lasso models can automatically do this by setting `normalize=True`.\n",
    "\n",
    "All features will be penalized equally when doing regularization. \n",
    "Consider what would happen for two features A and B that have equal predictive power but where B has really small values.\n",
    "Without regularization the model will have a large coefficient for B to also equally contribute to the output.\n",
    "However, regularization will punish B much more because its coefficient are so big and not because it has less predicitive power.\n",
    "Using an equal scale for the features levels the playing field.\n",
    "\n",
    "In addition, some optimization algorithms converge to better solutions if the input features are on the same range.\n",
    "Not that scaling doesn't always make sense: if you're units have the same scale (e.g. euro's) it might not make sense to scale them.\n",
    "\n",
    "\n",
    "#### Bonus question\n",
    "\n",
    "The documentation mentions that setting `normalize=True` is not the same as standarization.\n",
    "\n",
    "> What's the difference between normalization and standarization in this case? What different kinds of normalization can you find?\n",
    "\n",
    "Standardization is scaling a feature to have zero mean and unit variance.\n",
    "Normalization is scaling a vector to have unit norm.\n",
    "If the vector has no average, standardization and normalization only differ by a factor (`len(X)`).\n",
    "\n",
    "In different area's normalization can be used as a term for:\n",
    "\n",
    "- scaling the samples (for e.g. clustering);\n",
    "- min-max scaling (scale a vector in the range of [0, 1]);\n",
    "- ...?\n",
    "\n",
    "Complete the cell below to initialize and store the three regression models:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "from sklearn.linear_model import LassoCV, RidgeCV, LinearRegression\n",
    "\n",
    "# Initialize the three models\n",
    "lasso_reg = <FILL IN>\n",
    "ridge_reg = <FILL IN>\n",
    "linear_reg = <FILL IN>\n",
    "\n",
    "# Store the models in a dict\n",
    "models = dict(lasso=lasso_reg, ridge=ridge_reg, linear=linear_reg)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load ../answers/05_01_initialize.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Define scorers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will not use the models' default scoring metrics to measure the model performance. Instead, we will use two common metrics: negative mean absolute error (MAE) and negative mean squared error (MSE). \n",
    "\n",
    "Print the names of the all common scorers below to figure out how to specify the two scorers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import SCORERS\n",
    "\n",
    "# print the common scorer names\n",
    "print(\"SCORERS: {}\".format(list(SCORERS.keys())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "# Store the scorers' names as strings in a dict\n",
    "scorers = dict(mae=<FILL IN>, mse=<FILL IN>)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load ../answers/05_02_scorers.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Create cross-validation iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-validation strategy for scoring the three models will be 5-fold cross-validation (with shuffling); create the iterator:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# create 5-Fold iterator\n",
    "cv = <FILL IN>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load ../answers/05_03_5fold.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Perform model selection\n",
    "Now let's compute the cross-validated scores using the previously-defined models, scorers and cross-validation iterators and store the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# create a DataFrame for storing the results\n",
    "index = pd.Index([(scorer, model) for scorer in scorers.keys() for model in models.keys()],\n",
    "                 name=['scorer','model'])\n",
    "columns = [\"split_{}\".format(split) for split in range(cv.n_splits)]\n",
    "results = pd.DataFrame(columns=columns, index=index)\n",
    "\n",
    "# evaluate the models with the scorers\n",
    "for scorer in scorers.keys():\n",
    "    for model in models.keys():\n",
    "        scoring = scorers[scorer]\n",
    "        current = models[model]\n",
    "        # compute cross-validated score\n",
    "        results.loc[(scorer,model),:] = cross_val_score(\n",
    "            current, X, y, scoring=scoring, cv=cv\n",
    "        )\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5  Evaluate model performance\n",
    "\n",
    "Visualize the results to select the best model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5.5))\n",
    "for idx, scorer in enumerate(scorers.keys()):\n",
    "    results.T[scorer].plot(kind='box', title=scorer, ax=axes[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Which model performs best for which metric?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Persist optimal model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the model that performs best.\n",
    "\n",
    "Before we can store this model, we have to train it first on the full dataset: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "reg = <FILL IN>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load ../answers/05_04_full.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, store the regression model on disk: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "from sklearn.externals import joblib\n",
    "file_name = 'boston_best_model.pkl'\n",
    "joblib.<FILL IN>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load ../answers/05_05_dump.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Rerun the cells in section 1.4 and 1.5 a few times, what is happening?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 2: Classification metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will play around several classification metrics. These metrics will be applied to the classifiction results for a noisier version of the digits dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the digits dataset\n",
    "X, y = convert_to_pandas(datasets.load_digits())\n",
    "\n",
    "# add some white noise\n",
    "np.random.seed(10)\n",
    "X = np.abs(X + np.random.randn(*X.shape) * 7)\n",
    "\n",
    "from lib.processing_functions import display_digits\n",
    "fig = display_digits(X, y)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Split the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into a single train and test set, use `random_state=7`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split the data set into a single train and test set\n",
    "X_train, X_test, y_train, y_test = <FILL IN>\n",
    "print(X_train.shape, X_test.shape)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load ../answers/05_06_split.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Fit random forest classifier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the `RandomForestClassifier` model to the data, perform grid search to optimize the `n_estimators` parameter (note, keep `n_estimators<=200`; moreover, to speed-up the fitting process you can try to modify the `n_jobs` parameter):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# specify the parameter grid\n",
    "param_grid = {'n_estimators': <FILL IN>}\n",
    "\n",
    "# perform the gridsearch using the param_grid\n",
    "grid_clf = <FILL IN>\n",
    "\n",
    "# fit the model on train data\n",
    "grid_clf.<FILL IN>\n",
    "\n",
    "# make predictions on test data\n",
    "y_pred = grid_clf.<FILL IN>\n",
    "\n",
    "# get prediction probabilities\n",
    "y_score = grid_clf.<FILL IN>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load ../answers/05_07_fit_rf.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Default model score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start simple by printing the default model score for the test set:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "# compute default score on the test set\n",
    "accuracy = grid_clf.<FILL IN>\n",
    "\n",
    "print(\"accuracy: {}\".format(accuracy))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load ../answers/05_08_default_score.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze the performance of the classifier by plotting the normalized confusion matrix, showing the predicted label percentages for each true label:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# compute confusion matrix\n",
    "cm = <FILL IN>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load ../answers/05_09_confusion.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize such that rows sum to 1\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "plt.imshow(cm_normalized, cmap='Blues', interpolation='nearest')\n",
    "plt.grid(False)\n",
    "plt.ylabel('true')\n",
    "plt.xlabel('predicted')\n",
    "plt.xticks(range(10))\n",
    "plt.yticks(range(10))\n",
    "plt.title(\"Normalized confusion matrix\")\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Which noisy digits seems hardest to predict and which one easiest?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Classification report:\n",
    "To get a quick overview of the classification performance for each class print the classification report:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# print the classification report\n",
    "print(classification_report(y_test, y_pred, labels=range(10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: What does it mean that the precision for digit 8 is still ok but the recall is very low?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 One-versus-rest classification\n",
    "Let's have a look at two one-versus-rest transformations of the problem, turning the problem into two binary classification tasks of\n",
    "\n",
    "- is this sample an 8 or is it not an 8\n",
    "- is this sample a 5 or is it not a 5\n",
    "\n",
    "For both these problems we will plot the receiver operator characteristic (ROC) and compute the area under the curve (AUC).\n",
    "\n",
    "First create a vector which has a 1 if the `y_test==8` and 0 if `y_test!=8`, then do the same for digit 5:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "# convert labels into binary classification problem\n",
    "y_test_8 = (y_test == 8).astype(float)\n",
    "y_test_5 = <FILL IN>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load ../answers/05_10_convert.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, we need the probability scores for each sample; these scores provide the probabilities of the sample being one of the 10 possible labels. The earlier computed  `y_score` contains these prediction probability scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select sample\n",
    "idx = 29\n",
    "\n",
    "# get sample probability scores\n",
    "sample_prob = y_score[idx, :]\n",
    "\n",
    "# plot label prediction probabilities\n",
    "prob_df = pd.DataFrame({'prob': sample_prob}).rename_axis('digit')\n",
    "title = 'sample #{} probability scores - true label={}'\n",
    "prob_df.plot(kind='bar', title=title.format(idx, y_test.iloc[idx]), rot=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: What digit is sample 29 (`idx=29`), and what probability did our model give to it being that digit?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the pobability scores of predicting the digit 8 and do the same for digit 5:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "# assign probability score of predicting digit\n",
    "y_score_8 = y_score[:, 8] \n",
    "y_score_5 = <FILL IN>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load ../answers/05_11_score.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the binary labels and digit probability scores to create ROC curves for both digits and compute the area under the curve:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "fpr, tpr, roc_auc = dict(), dict(), dict()\n",
    "\n",
    "fpr[8], tpr[8], _ = roc_curve(y_test_8, y_score_8)\n",
    "roc_auc[8] = auc(fpr[8], tpr[8])\n",
    "\n",
    "fpr[5], tpr[5], _ = <FILL IN>\n",
    "roc_auc[5] = <FILL IN>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load ../answers/05_12_roc.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fpr[8], tpr[8], label='digit 8: ROC curve (AUC = %0.2f)' % roc_auc[8])\n",
    "plt.plot(fpr[5], tpr[5], label='digit 5: ROC curve (AUC = %0.2f)' % roc_auc[5])\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='luck')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc=4, fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Interpret the results, does it match the conclusions we drew from in the confusion matrix? What AUC would we get when the classifier just makes random predictions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Prediction results\n",
    "Run the cell below to visualize 40 of the samples in the test set. If the label is red than the classfier make a classification error:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = display_digits(X_test, y_test, y_pred, n_max=40) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classifying these digits is not easy, is it?!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load ../answers/05_questions.py"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
