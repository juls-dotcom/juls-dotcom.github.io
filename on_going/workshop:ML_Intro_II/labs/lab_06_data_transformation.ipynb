{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 06 - Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn import datasets\n",
    "%matplotlib inline\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from lib.processing_functions import convert_to_pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise goals:\n",
    "\n",
    "- practice with building data pipelines\n",
    "- perform feature extraction from text data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 1: Builing a pipeline\n",
    "\n",
    "We revisit the Boston dataset to see if adding extra polynomial terms to the data improves the prediction performance of a `Lasso` model. We will build a pipeline for figuring this out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the Boston dataset\n",
    "X, y = convert_to_pandas(datasets.load_boston())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Initialize estimators objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two preprocessing steps will be performed before fitting the data: standardization of data and addition of polynomial terms.\n",
    "\n",
    "Let's start with creating the `StandardScaler` and `PolynomialFeatures` transformers.\n",
    "Initialize both of them with `include_bias=False`: transformer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "# import the StandardScaler class\n",
    " <FILL IN>\n",
    "\n",
    "# initialize standard scaler\n",
    "scaler = <FILL IN>\n",
    "\n",
    "\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "# import the PolynomialFeatures class\n",
    "<FILL IN>\n",
    "\n",
    "# intialize polynomial feature creator\n",
    "polynomial = <FILL IN>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load ../answers/06_01_initialize.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the Lasso estimator (in contrast to the previous lab, we will not use the one with build-in cross-validation); note that normalization is not necessary due to the standard scaling preprocessing step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Initialize the lasso estimator\n",
    "lasso = Lasso()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Create the pipeline\n",
    "\n",
    "Combine the preprocessing transformers and estimator in a pipeline:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# create the pipeline object\n",
    "pipeline = Pipeline([('scaler', <FILL IN>),\n",
    "                     ('polynomial', <FILL IN>),\n",
    "                     ('lasso', <FILL IN>)])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load ../answers/06_02_pipeline.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Optimize with grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now perform the grid search over the specified parameters of objects in the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# choose hyperparameter grid\n",
    "param_grid = {'lasso__alpha': np.logspace(-.25,1,10),\n",
    "              'polynomial__degree': [1,2,3]}\n",
    "\n",
    "# perform cross-validated gridsearch \n",
    "grid_pipeline = GridSearchCV(pipeline, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "# fit the data\n",
    "grid_pipeline.fit(X, y)\n",
    "\n",
    "# detimine best parameters\n",
    "grid_pipeline.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: For which objects in the pipeline did we optimize hyperparameters, and what cross-validation scoring strategies were used? Did adding the polynomial features to our data make sense?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Estimated model parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the estimated model parameters of the best Lasso estimator resulting from the grid search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the `Lasso` estimator from the best estimator of the `grid_pipeline` object (hint: look up how get an estimator from a pipeline by naming the step):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "best_pipeline = grid_pipeline.<FILL IN>\n",
    "best_lasso = best_pipeline.<FILL IN>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load ../answers/06_03_best.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the second degree polynomial features to the data increases the number of features. Determine the number of features from the shape of the `coef_` vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = best_lasso.coef_.shape\n",
    "print(\"number of features: {}\".format(n_features[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this many features it important to prevent overfitting. Lasso protects against overfitting by applying 'l1' regularization. A nice property of this type of regularization is that the resulting models are sparse, meaning many coefficient are zero. \n",
    "\n",
    "Let's visualize the estimated model coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(best_lasso.coef_, columns=['coef']).plot(kind='Bar', figsize=(15,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: The visualization show that around 20% of the 104 features are non-zero, what would be an advantage of such an sparse model? Does 'l2' regularization (used in Ridge regression) also result in sparse models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 2: Text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train a classifier on text data to determine whether a comment contains an insult or not.\n",
    "\n",
    "The labeled data is split into train and test datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the insult datasets\n",
    "train_data = pd.read_csv(\"../data/insult_train.csv\") \n",
    "test_data = pd.read_csv(\"../data/insult_test.csv\") \n",
    "\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Extract features and targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the targets from the 'Insult' column and the features from the 'Comment' column of the dataframe:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "# create X, y data sets for both testing and training\n",
    "text_train = <FILL IN>\n",
    "y_train = <FILL IN>\n",
    "text_test = <FILL IN>\n",
    "y_test = <FILL IN>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load ../answers/06_04_extract.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the `TfidfVectorizer` transformer to convert the text to feature vectors that can be fed to a classification algorithm. \n",
    "\n",
    "This transformer outputs sparse feature vectors containing token counts (=word counts). These counts have been normalized based on how often the tokens occur in the different documents (=comments), this is called tf-idf. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the `TfidfVectorizer` transformer. \n",
    "Fit the train text and check-out the size of the built vocabulary:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "# import tranformer classs\n",
    "<FILL IN>\n",
    "\n",
    "# initialize the countvectorizer\n",
    "vectorizer = <FILL IN>\n",
    "\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "# fit the train text\n",
    "vectorizer.<FILL IN>\n",
    "\n",
    "# print vocabulary size\n",
    "vocabulary_size = len(vectorizer.<FILL IN>)\n",
    "\n",
    "print('vocabulary size: {}'.format(vocabulary_size))\n",
    "\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "# transform train text\n",
    "X_train = vectorizer.<FILL IN>\n",
    "#transform test text\n",
    "X_test = vectorizer.<FILL IN>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load ../answers/06_05_vectorizer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Fit a linear SVC model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit a `LinearSVC` estimator to the train data.\n",
    "Compute the score on the test set using the estimator's default scoring method:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "# import the estimator class\n",
    "<FILL IN>\n",
    "\n",
    "# initialize the classifier\n",
    "clf = <FILL IN>\n",
    "\n",
    "# fit the train data\n",
    "clf.<FILL IN>\n",
    "\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "# compute accuracy on test set\n",
    "accuracy = clf.<FILL IN>\n",
    "\n",
    "print(\"accuracy {}:\".format(accuracy))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load ../answers/06_06_svc.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Estimated model parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the function below to visualize the top features used by fitted model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_coefficients(classifier, feature_names, n_top_features=20):\n",
    "    # get coefficients with large absolute values \n",
    "    coef = classifier.coef_.ravel()\n",
    "    positive_coefficients = np.argsort(coef)[-n_top_features:]\n",
    "    negative_coefficients = np.argsort(coef)[:n_top_features]\n",
    "    interesting_coefficients = np.hstack([negative_coefficients, positive_coefficients])\n",
    "    # plot them\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    colors = [\"red\" if c < 0 else \"blue\" for c in coef[interesting_coefficients]]\n",
    "    plt.bar(np.arange(2*n_top_features), coef[interesting_coefficients], color=colors)\n",
    "    feature_names = np.array(feature_names)\n",
    "    plt.xticks(np.arange(1, (2*n_top_features)+1), feature_names[interesting_coefficients], \n",
    "               rotation=60, ha=\"right\", fontsize=16)\n",
    "    plt.ylabel('insult contribution')\n",
    "    max_coef = np.abs(max(coef[interesting_coefficients]))\n",
    "    plt.ylim(-max_coef*1.1, max_coef*1.1)\n",
    "    \n",
    "visualize_coefficients(clf, vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Does the insult contribution of the coefficients make sense?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Pipeline with grid search "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Combine the vectorizer and classfier into a data pipeline\n",
    "- Perform grid search over the predefined parameters (note this can take a while!)\n",
    "- Compute the score on the test set using the estimator's default scoring method:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# create pipeline out of the TfidfVectorizer and LinearSVC\n",
    "pipeline = Pipeline([('tv', <FILL IN>), ('svc', <FILL IN>)])\n",
    "\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# defined hyperparameter grid\n",
    "param_grid = {'svc__C': [0.5,1,2],\n",
    "              'tv__ngram_range': [(1,1),(1,2)]}\n",
    "\n",
    "# create gridsearch object\n",
    "grid_pipeline = <FILL IN>\n",
    "\n",
    "# fit the text and y train data\n",
    "grid_pipeline.<FILL IN>\n",
    "\n",
    "# display best hyperparameters\n",
    "grid_pipeline.<FILL IN>\n",
    "\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "# comput accuracy on test text (note not test set; but test text!)\n",
    "accuracy = grid_pipeline.<FILL IN>\n",
    "\n",
    "print(\"accuracy {}:\".format(accuracy))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load ../answers/06_07_pipeline.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Did our grid search result in performance gain on the test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load ../answers/06_questions.py"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
