{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 04 - Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn import datasets\n",
    "%matplotlib inline\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from lib.processing_functions import convert_to_pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise goals:\n",
    "\n",
    "- Get a feeling how cross-validation iterators split the data.\n",
    "- Learn how to compute cross-validated predictions and scores.\n",
    "- Practice hyperparameter optimization using grid search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 1: Cross-validation iterators\n",
    "\n",
    "We will visualize the splits generated by different cross-validation iterators.\n",
    "These iterators are explained in more detail in [3.1.2. Cross validation iterators](http://scikit-learn.org/stable/modules/cross_validation.html#cross-validation-iterators).\n",
    "\n",
    "The iterators will be demonstrated on the Iris dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the Iris dataset\n",
    "X, y = convert_to_pandas(datasets.load_iris())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions below can be used for visualizing train and test sets generated by  cross-validation iterator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "def plot_labels(y, n_split, title):\n",
    "    \"\"\"Visualize the order of the original labels.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y : 1d array-like\n",
    "        Ground truth (correct) labels.\n",
    "    n_split : int\n",
    "        Number of equal splits.\n",
    "    title : str\n",
    "        Title of the plot.\n",
    "    \"\"\"\n",
    "    f, ax = plt.subplots(figsize=(15,1))\n",
    "    labels = np.tile(LabelEncoder().fit_transform(y), [n_split, 1])\n",
    "    ax.imshow(labels, interpolation='nearest', aspect='auto')\n",
    "    ax.grid(None)\n",
    "    ax.set_yticks([0])\n",
    "    ax.set_title(title, y=1.5, size=14)\n",
    "\n",
    "    \n",
    "def plot_cv(cv, X, y, title):\n",
    "    \"\"\"Visualize the order for a cross-validation scheme.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    cv : CrossValidator\n",
    "        Cross-validation iterator.\n",
    "    y : 1d array-like\n",
    "        Ground truth (correct) labels.\n",
    "    n_split : int\n",
    "        Number of equal splits.\n",
    "    title : str\n",
    "        Title of the plot.    \n",
    "    \"\"\"\n",
    "    f, ax = plt.subplots(figsize=(15,1), sharex=True)\n",
    "    masks = []\n",
    "    n_samples = len(y)\n",
    "    for train, test in cv.split(X, y):\n",
    "        mask = np.zeros(n_samples, dtype=bool)\n",
    "        mask[test] = 1\n",
    "        masks.append(mask)\n",
    "    ax.imshow(masks, interpolation='nearest', aspect='auto')\n",
    "    ax.set_xlabel('sample')\n",
    "    ax.set_yticks(range(0, cv.n_splits))\n",
    "    ax.set_yticklabels(range(1, cv.n_splits+1))\n",
    "    ax.grid(None)\n",
    "    ax.set_title(title, y=1.5, size=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the following three cross-validation iterators: \n",
    "\n",
    "- 5-fold cross-validation\n",
    "- stratified 5-fold cross-validation \n",
    "- shuffle split with a test set size 20% of the dataset and 5 iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "# import the cv iterators\n",
    "<FILL IN>\n",
    "\n",
    "# number of samples\n",
    "n_samples = <FILL IN>\n",
    "\n",
    "#create 5-Fold\n",
    "cv_5fold = <FILL IN>\n",
    "\n",
    "#create statified 5-Fold\n",
    "cv_strat_5fold = <FILL IN>\n",
    "\n",
    "#create shuffle split .2 test set size and 5 iterations\n",
    "cv_shuffle = <FILL IN>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load ../answers/04_01_cv.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now visualize the splits using the `plot_cv` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_labels(y, 5, 'Labels (colours)')\n",
    "plot_cv(cv_5fold, X, y, '5-fold')\n",
    "plot_cv(cv_strat_5fold, X, y, 'Stratified 5-fold')\n",
    "plot_cv(cv_shuffle, X, y, 'Shuffle split')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Are the black samples in the visualization the test or the train samples? Which method uses the same test samples in multiple splits?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also intriguing is the difference between regular 5-fold and stratified 5-fold splits. To explain this, let's compare the test set label counts for both methods:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_label_counts(X, y, cv, name):\n",
    "    \"\"\"Count the number labels for each fold.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 2d array-like\n",
    "        Features.\n",
    "    y : 1d array-like\n",
    "        Ground truth (correct) labels.\n",
    "    cv : CrossValidator\n",
    "        Cross-validation iterator.\n",
    "    name : str\n",
    "        Name of the cross-validation iterator.\n",
    "    \"\"\"\n",
    "    label_counts = pd.DataFrame(columns=y.unique())\n",
    "    for i, split in enumerate(cv.split(X, y)):\n",
    "        train, test = split\n",
    "        split_name = '{} test_set_{}'.format(name, i)\n",
    "        label_counts.loc[split_name,:] = y[test].value_counts()\n",
    "    return label_counts.fillna(0)\n",
    "\n",
    "print(test_label_counts(X, y, cv_5fold, '5-fold'))\n",
    "print(test_label_counts(X, y, cv_strat_5fold, 'stratified 5-fold'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: What is happening here? Would this problem become less severe is we would run regular K-Fold with `shuffle=True`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 2: Cross-validated scoring and prediction\n",
    "\n",
    "Let's compare the cross-validation scores and predictions of two different regression methods on the Boston dataset. We'll compare `LinearRegression` and `RandomForestRegressor`.\n",
    "\n",
    "More information on cross-validation scores and predictions can be found [3.1.1. Computing cross-validated metrics](http://scikit-learn.org/stable/modules/cross_validation.html#computing-cross-validated-metrics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the Boston dataset\n",
    "X, y = convert_to_pandas(datasets.load_boston())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Cross-validated scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cross-validation scorers can be used to quickly get a cross-validated score of a model and see if it is able to learn something.\n",
    "We'll use it to assess the performance of `LinearRegression` and `RandomForestRegressor`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a 5-fold cross-validation iterator with shuffling: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# create 5-Fold iterator with shuffle\n",
    "cv = <FILL IN>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load ../answers/04_02_5fold.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the cross-validated score for the `LinearRegression` estimator using the previously-created iterator `cv`: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "from sklearn.model_selection import <FILL IN>\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# compute the cross-validated score for the LinearRegression \n",
    "lr_scores = <FILL IN>\n",
    "\n",
    "print(\"cv scores: {}\".format(lr_scores))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load ../answers/04_03_lr_scores.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the same for the `RandomForestRegressor` estimator:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# compute the cross-validated score for the RandomForestRegressor \n",
    "rf_scores = <FILL IN>\n",
    "\n",
    "print(\"cv scores: {}\".format(rf_scores))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load ../answers/04_04_rf_scores.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show boxplots of both results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame([lr_scores, rf_scores], \n",
    "                       index=['linear regression', 'random forest regressor'],\n",
    "                       columns = [\"split_{}\".format(k) for k in range(cv.n_splits)]).T\n",
    "ax = results.boxplot()\n",
    "ax.set_ylabel('$R^2$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Which model performs best? Is this the best that both models can do, or do you see opportunites to improve one or both?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Cross-validated predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cross-validation predictor `cross_val_predict` runs cross-validation and returns the predictions for the data points when they are in the test fold.\n",
    "This is a quick way to get predictions from our model and see if it has any clear biases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the cross validated predictions for the `LinearRegression` and `RandomForestRegressor` estimators using the `cv` iterator created in 2.1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "from sklearn.model_selection import <FILL IN>\n",
    "\n",
    "# compute the cross-validated predictions for the LinearRegression \n",
    "lr_y_pred = <FILL IN>\n",
    "# compute the cross-validated predictions for the RandomForestRegressor \n",
    "rf_y_pred = <FILL IN>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load ../answers/04_05_xval_predictions.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `truth_vs_predictions` function below to plot `y` versus `y_pred`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truth_vs_prediction(y, y_pred, ax=None, title=None):\n",
    "    \"\"\"Plot truth versus predictions.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : 1d array-like\n",
    "        Ground truth (correct) labels.\n",
    "\n",
    "    y_pred : 1d array-like\n",
    "        Predicted labels, as returned by a classifier.\n",
    "        \n",
    "    ax : matplotlib.axes\n",
    "        Axes to plot on.\n",
    "        \n",
    "    title : str\n",
    "        Title for plot.\n",
    "    \"\"\"\n",
    "    if ax is None:\n",
    "        ax = plt.subplot()\n",
    "    y_max = y.max()\n",
    "    ax.plot(y, y_pred, 'o', markersize=2)\n",
    "    ax.plot([0,y_max], [0,y_max],':r')\n",
    "    ax.set_xlabel('Truth')\n",
    "    ax.set_ylabel('Prediction')\n",
    "    ax.set_xlim(0,y_max)\n",
    "    ax.set_ylim(0,y_max)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_title(\"truth versus prediction\" if title is None else title)\n",
    "\n",
    "# plots the truth versus cv predictions for both regressors\n",
    "fig ,axes = plt.subplots(1, 2, figsize=(8,4), sharey=True)\n",
    "fig.set_size_inches(11.0, 8.5)\n",
    "truth_vs_prediction(y, lr_y_pred, axes[0], 'linear regression')\n",
    "truth_vs_prediction(y, rf_y_pred, axes[1], 'random forest regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Does the random forest regressor suffer from the same systematic error as the linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 3: Hyperparameter optimization\n",
    "\n",
    "We shouldn't just use the default hyperparameters of our models: the defaults might not be good choices for our problem!\n",
    "\n",
    "Let's return to the digits dataset and optimize the Linear SVM model fitted in Lab 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the digits dataset\n",
    "X, y = convert_to_pandas(datasets.load_digits())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Cross-validated grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a stratified 5-fold cross-validation iterator with shuffling: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# create 5-Fold iterator with shuffle\n",
    "cv = <FILL IN>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load ../answers/04_06_stratified.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Lab 2 we ran `LinearSVC` with an 'l2' penalty and `C=1.0`. The penalty refers to [regularization](http://scikit-learn.org/stable/auto_examples/svm/plot_svm_scale_c.html) of the model. \n",
    "\n",
    "Perform a cross-validated grid search over the `C` parameter that regulates the amount of regularization. Use the following grid values [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1e0, 1e1] and the previously created cross-validation iterator `cv`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import <FILL IN>\n",
    "\n",
    "# specify the parameter grid\n",
    "param_grid = {'C': <FILL IN>}\n",
    "\n",
    "# perform the gridsearch using the cv-iterator and param_grid\n",
    "grid_clf = <FILL IN>\n",
    "\n",
    "# fit the data\n",
    "grid_clf.<FILL IN>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load ../answers/04_07_grid_search.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the cross validation results for the different values of `C`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# display the grid scores\n",
    "grid_clf.<FILL IN>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load ../answers/04_08_grid_scores.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Did we pick a large enough parameter range?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2  Validation curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a bit more insight in how cross-validated grid search works, we can plot a validation curve. This plot shows both the training and cross-validation scores for the different hyperparameter candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "train_scores, valid_scores = validation_curve(LinearSVC(), X, y, 'C', param_grid['C'])\n",
    "best_C = grid_clf.best_params_['C']\n",
    "\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "valid_scores_mean = np.mean(valid_scores, axis=1)\n",
    "valid_scores_std = np.std(valid_scores, axis=1)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(11.0, 8.5)\n",
    "ax.set_title(\"Validation Curve with Linear SVM\")\n",
    "ax.set_xlabel(\"C\")\n",
    "ax.set_ylabel(\"Score\")\n",
    "ax.set_ylim(0.8, 1.1)\n",
    "ax.semilogx(param_grid['C'], train_scores_mean, label=\"Training score\", color=\"r\")\n",
    "ax.fill_between(param_grid['C'], train_scores_mean - train_scores_std,\n",
    "                train_scores_mean + train_scores_std, alpha=0.2, color=\"r\")\n",
    "ax.semilogx(param_grid['C'], valid_scores_mean, label=\"Cross-validation score\", \n",
    "            color=\"g\")\n",
    "ax.fill_between(param_grid['C'], valid_scores_mean - valid_scores_std,\n",
    "                 valid_scores_mean + valid_scores_std, alpha=0.2, color=\"g\")\n",
    "ax.plot([best_C,best_C], [0.8,1.1], 'k:', label='best C')\n",
    "ax.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**:  Which curve relates to the (very good) score we obtained in Lab 2? Consider the scikit-learn documentation on [learning curves](http://scikit-learn.org/stable/modules/learning_curve.html) to get more information!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load ../answers/04_questions.py"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
