{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from lib.processing_functions import convert_to_pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Outline\n",
    "\n",
    "Goal: Understand modeling workflow and explain steps for model selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Topics:\n",
    "\n",
    "- **Modeling workflow**: overview of the workflow\n",
    "- **Model validation**: methodology for checking how well a model fits a given dataset\n",
    "- **Hyperparameter optimization**: choosing a set of model parameters that optimize a model's performance (grid search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##  Modeling workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In summary, an example workflow for training a predictive model consist of the following steps:\n",
    "\n",
    "- **data splitting**: split the dataset into a train/validation set and a test set\n",
    "- **model selection**: fit the most optimal model to the train data using grid search (*includes validation*)\n",
    "- **model evaluation**: evaluate the performance of the final model\n",
    "\n",
    "The output of this workflow is a model that can be used for prediction and/or inference.\n",
    "\n",
    "In this notebook we focus on the **model selection** part. Since in the previous notebook we have already studied model fitting, we continue with model validation before we explain grid search. The next notebook will cover model evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"../images/example_workflow.png\" style=\"display: block;margin-left: auto;margin-right: auto;width: 600px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model validation\n",
    "\n",
    "We should not test model performance on data used for training: \n",
    "\n",
    "- complex models tend to **overfit** the training data;\n",
    "- overfitted models **don't generalize** well to yet-unseen data;\n",
    "- so we should validate on unseen data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Validation\n",
    "\n",
    "The solution is to split the data into different data sets:\n",
    "\n",
    "- **train set**: used for training the model\n",
    "- **validation set**: used for hyperparameter optimization (if necessary)\n",
    "- **test set**: used for evaluating the final model \n",
    "\n",
    "<img src=\"../images/data_splitting.png\" style=\"display: block;margin-left: auto;margin-right: auto;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "###  Holdout validation\n",
    "\n",
    "Simple dataset splitting is often problematic:\n",
    "* we want to learn from as much data as possible\n",
    "* we also want to test on a large data set\n",
    "* taking a single partitioning might not be very robust"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### K-fold cross-validation\n",
    "\n",
    "Split the data into $k$ independent folds: \n",
    "\n",
    "- fit the model on $kâˆ’1$  train folds\n",
    "- estimate performance on the remaining fold\n",
    "- repeat this procedure  $k$  times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![cv](../images/cross_validation_E.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hyperparameter optimization\n",
    "\n",
    "Optimization of parameters that are not directly learned within an estimator:\n",
    "* Model hyperparameters specify *how* the model learns.\n",
    "\n",
    "Hyperparameters are optimized by changing them and observing the (cross-) validation score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Examples of hyperparameters:\n",
    "\n",
    "- Regularization strength for LASSO and Ridge regression\n",
    "- Number and depth of trees in random forest algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "###  Pathology of a hyperparameter search\n",
    "\n",
    "The search for optimal hyperparameters consists of the following components:\n",
    "\n",
    "- a hyperparameter space\n",
    "- a method for searching or sampling candidates\n",
    "- a cross-validation scheme\n",
    "- a score or evaluation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Grid search\n",
    "\n",
    "Brute-force search over a set of possible hyperparameter candidates.\n",
    "\n",
    "- Specify a list of values for different hyperparameters.\n",
    "- Evaluate the model performance for every possible parameter combination using cross-validation.\n",
    "- Optimal hyperparameter set: highest performance score.\n",
    "- Evaluate your best performing model on the test data.\n",
    "- Build your final production model on the whole dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Hyperparameter search &  cross-validation\n",
    "\n",
    "![hs](../images/hyperparameter-search.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tools for model selection\n",
    "\n",
    "Scikit-learn has several tools for efficiently doing dataset splitting:\n",
    "\n",
    "- **`train_test_split`**: simple splitting of a dataset into a train and test set\n",
    "- **`cross_val_score`**: evaluate score by cross-validation \n",
    "- **`permutation_test_score`**: evaluate the significance of a cross-validated score\n",
    "- **`cross_val_predict`**: generates estimates by cross-validation\n",
    "- **`cross-validation iterators`**: large set of iterators which generate dataset splits according to different strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Next we will discuss some different tools in more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Simple dataset splitting\n",
    " \n",
    "Function for splitting the dataset into a train and a test set:\n",
    "\n",
    "```model_selection.train_test_split(*arrays, test_size=None, train_size=None, random_state=None)```\n",
    "\n",
    "Usage details:\n",
    "- an arbitrary number of arrays can be split\n",
    "- `test_size`/`train_size` can be specified as\n",
    "    - `None` : automatically complement the other split (default is 25% test, 75% train)\n",
    "    - `float` : proportion of the dataset to be included\n",
    "    - `int` : absolute number of samples to be included\n",
    "- set the `random_state` to make splitting deterministic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Dataset splitting example\n",
    "Sample a training set from the Iris dataset while withholding 40% of the data for testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (150, 4), y shape: (150,)\n",
      "X_train shape: (90, 4), y_train shape: (90,)\n",
      "X_test shape: (60, 4), y_test shape: (60,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = convert_to_pandas(datasets.load_iris())\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, \n",
    "                                                    random_state=0)\n",
    "\n",
    "print(\"X shape: {}, y shape: {}\".format(\n",
    "    X.shape, y.shape))\n",
    "print(\"X_train shape: {}, y_train shape: {}\".format(\n",
    "    X_train.shape, y_train.shape))\n",
    "print(\"X_test shape: {}, y_test shape: {}\".format(\n",
    "    X_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cross-validated metrics\n",
    "\n",
    "Function for evaluating a metric score by cross-validation:\n",
    "\n",
    "   `model_selection.cross_val_score(estimator, X, y=None, scoring=None, cv=None, n_jobs=1, fit_params=None, ...)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Cross-validated accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Compute cross-validated accuracy of a logistic regression model on the Iris dataset by 5-Fold cross-validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores per fold: [ 0.76666667  0.86666667  0.83333333  0.83333333  0.8       ]\n",
      "Mean accuracy: 0.82\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "clf = LogisticRegression(C=0.1)\n",
    "\n",
    "scores = cross_val_score(clf, X, y, cv=5, scoring='accuracy')\n",
    "\n",
    "print(\"Scores per fold: {}\".format(scores))\n",
    "print(\"Mean accuracy: {0:.2f}\".format(scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cross-validation iterators\n",
    "\n",
    "By specifying an integer $k$ for the `cv` parameter we get the default $k$-fold cross-validation. We can also be more specific on the type of cross-validation that we want to perform by creating a cross-validation iterator.\n",
    "\n",
    "Many different cross-validation iterators are supported:\n",
    "    - `StratifiedKFold`\n",
    "    - `ShuffleSplit`\n",
    "    - `LeavePOut`\n",
    "    - ...\n",
    "\n",
    "Cross-validation iterators generate sets of train and test indices that can be used for splitting the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**StratifiedKFold**: for each fold/split the original class ratios are preserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class ratio dataset:\n",
      "0.25\n",
      "\n",
      "stratified splits:\n",
      "[{'train': 0.25, 'test': 0.25}, {'train': 0.25, 'test': 0.25}, {'train': 0.25, 'test': 0.25}]\n",
      "\n",
      " regular splits:\n",
      "[{'train': 0.25, 'test': 0.25}, {'train': 0.375, 'test': 0.0}, {'train': 0.125, 'test': 0.5}]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "\n",
    "X_data = np.array([0,0,0,0,0,0,0,0,0,0,0,0])\n",
    "y_data = np.array([0,0,0,0,0,0,0,0,0,1,1,1])\n",
    "\n",
    "stratifier = StratifiedKFold(n_splits=3, shuffle=True, random_state=1)\n",
    "kfolder = KFold(n_splits=3, shuffle=True, random_state=1)\n",
    "\n",
    "stratified_splits = stratifier.split(X_data, y_data)\n",
    "regular_splits = kfolder.split(X_data, y_data)\n",
    "\n",
    "def label_ratios(splits):\n",
    "    \"determine class label ratios for all splits\"\n",
    "    return [\n",
    "        {label: sum(y_data[dset])/len(y_data[dset]) \n",
    "         for label, dset in zip(('train', 'test'), split)}\n",
    "        for split in splits\n",
    "    ]\n",
    "\n",
    "print(\"class ratio dataset:\\n{}\".format(sum(y_data)/len(y_data)))\n",
    "print(\"\\nstratified splits:\\n{}\".format(label_ratios(stratified_splits)))\n",
    "print(\"\\n regular splits:\\n{}\".format(label_ratios(regular_splits)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**LeaveOneGroupOut**: hold out the samples corresponding to $p$ labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 train: [2 3 4 5 6]\ttest: [0 1]\ttest labels: [2010, 2010]\n",
      "2 train: [0 1 5 6]\ttest: [2 3 4]\ttest labels: [2011, 2011, 2011]\n",
      "3 train: [0 1 2 3 4]\ttest: [5 6]\ttest labels: [2012, 2012]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "\n",
    "x_data = range(7)\n",
    "labels = [2010, 2010, \n",
    "          2011, 2011, 2011, \n",
    "          2012, 2012]\n",
    "\n",
    "logo = LeaveOneGroupOut()\n",
    "logo_splits = logo.split(x_data, groups=labels)\n",
    "\n",
    "for i, split in enumerate(logo_splits):\n",
    "    print(f\"{i+1} train: {split[0]}\\ttest: {split[1]}\\ttest labels: {[labels[x] for x in split[1]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Data shuffling\n",
    "\n",
    "Data is often ordered and shuffling may be essential to get a meaningful cross-validation result.\n",
    "\n",
    "Note that the opposite may also be true! This is usually the case when working with time series data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Some iterators, like `KFold`, allow shuffling of the data before splitting. However, note that:\n",
    "\n",
    "- by default **no shuffling** occurs\n",
    "- `random_state` needs to be set to make it deterministic.\n",
    "\n",
    "Other iterators inherently shuffle the data, for example `ShuffleSplit`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Grid search example\n",
    "\n",
    "Optimize both the type and the amount of regularization for the logistic regression model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid size: 20\n",
      "best score: 0.9809523809523809\n",
      "best estimator: LogisticRegression(C=166.81005372000558, class_weight=None, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)\n",
      "Evaluation score on test set: 0.9777777777777777\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state=0)\n",
    "param_grid = {\n",
    "    'C': np.logspace(-4.0, 4.0, num=10), \n",
    "    'penalty': ['l1', 'l2'] \n",
    "}\n",
    "\n",
    "clf = GridSearchCV(LogisticRegression(), param_grid=param_grid, cv=5)\n",
    "clf = clf.fit(X_train, y_train)\n",
    "\n",
    "grid_size = np.product([len(value) for value in param_grid.values()])\n",
    "print(\"grid size: {}\".format(grid_size))\n",
    "print(\"best score: {}\".format(clf.best_score_))\n",
    "print(\"best estimator: {}\".format(clf.best_estimator_)) \n",
    "\n",
    "best_model = clf.best_estimator_.fit(X_train, y_train)\n",
    "evaluation_score = best_model.score(X_test, y_test)\n",
    "print(\"Evaluation score on test set: {}\".format(evaluation_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Estimator specific cross-validation\n",
    "\n",
    "Some of the estimators that have model selection via cross-validation built-in:\n",
    "\n",
    "- `ElasticNetCV`: Elastic Net model with iterative fitting along a regularization path\n",
    "- `LassoCV`: Lasso linear model with iterative fitting along a regularization path\n",
    "- `LogisticRegressionCV`: Logistic Regression CV (aka logit, MaxEnt) classifier\n",
    "- `RidgeCV`: Ridge regression with built-in cross-validation\n",
    "\n",
    "Most focus on optimizing the **strength of the regularizer**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Built-in cross-validation example\n",
    "\n",
    "Run the logistic regression with built-in cross-validation on the whole Iris data and compute the mean score: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegressionCV mean score: 0.9268790849673202\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "scores = cross_val_score(LogisticRegressionCV(), X, y)\n",
    "\n",
    "print(\"LogisticRegressionCV mean score: {}\".format(scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Advanced hyperparameter optimization: randomized search\n",
    "\n",
    "A probabalistic search over discrete or continuous ranges of hyperparameters. For each entry in a `param_grid` dict,\n",
    "\n",
    "+ if a discrete list of values is given, this list is sampled without replacement\n",
    "+ if a continuous distribution object is given (imported, say, from `scipy.stats`), values will be sampled with replacement following the particular form of the distribution; for instance: `param_grid={'alpha': scipy.stats.norm()}`\n",
    "\n",
    "Why prefer the randomized approach to uniformly searching each hyperparameter subspace? Empirically, it has been shown that often a variation in one or more hyperparameters has little effect on the generalizability of the resulting model. We see this in the following image:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](../images/randomized_search.png) Source: <a href=\"http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf\">J. Bergstra and Y. Bengio</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The scikit-learn API call is:\n",
    "\n",
    "```python\n",
    "from sklearn.grid_search import RandomizedSearchCV\n",
    "```\n",
    "\n",
    "In addition to the parameters `GridSearchCV` accepts, `RandomizedSearchCV` also accepts `n_iter`, the number of hyperparameter value choices which will be assessed. Using this parameter, we can choose a resource budget which is independent of the number of hyperparameters and the number of possible values they can take, essentially decoupling our runtime from our search space size. In this way, adding hyperparameters to `param_grid` doesn't *per se* influence the performance of the search or decrease its efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Review Questions \n",
    "\n",
    "1. What might go wrong if you only split test/train once? \n",
    "2. When might a gridsearch be a better/worse idea than a randomised search? \n",
    "3. Is it important to add a seed to our crossvalidation calls? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Exercises: [lab 4 - Model selection](../labs/lab_04_model_selection.ipynb)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "livereveal": {
   "start_slideshow_at": "selected",
   "transition": "convex"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
